---
title: The Annotated MOTR
date: 2024-04-7 23:52:00 +0800
img_path: /assets/img/
categories: [Machine Learning]
tags: [Computer Vision]     # TAG names should always be lowercase
---

# The Annotated MOTR

# Table of Contents

1. [Introduction](#1-introduction)
2. [MOTR](#2-motr)
    1. [_forward_single_image()](#21-_forward_single_image)
    2. [_post_process_single_image()](#22-_post_process_single_image)
3. [Query Interaction Module](#3-query-interaction-module)
    1. [forward()](#31-forward)
    2. [_select_active_tracks()](#32-_select_active_tracks)
    3. [_update_track_embedding()](#33-_update_track_embedding)
4. [Data Structure and Functions](#4-data-structure-and-functions)
    1. [Track Instance](#41-track-instance)
    2. [RuntimeTrackerBase](#42-runtimetrackerbase)
        <!-- 1. [\_\_init\_\_()](#421-__init__)
        2. [update()](#422-update) -->
    3. [CheckpointFunction](#43-checkpointfunction)
        <!-- 1. [torch.autograd.Function](#431-torchautogradfunction)
        2. [CheckpointFunction](#432-checkpointfunction) -->
    4. [Data Preparation](#44-data-preparation)
        <!-- 1. [build()](#441-build)
        2. [\_\_init\_\_()](#442-__init__)
        3. [\_\_getitem\_\_()](#443-__getitem__) -->

## 1. Introduction

## 2. MOTR

MOTR architecture is based on DETR and Deformable DETR. 

Here we directly check the `forward()` method in the `MOTR` class. 

```python
    def forward(self, data: dict):
        if self.training:
            self.criterion.initialize_for_single_clip(data['gt_instances'])
        frames = data['imgs']  # list of Tensor.
        outputs = {
            'pred_logits': [],
            'pred_boxes': [],
        }

        track_instances = self._generate_empty_tracks()
        keys = list(track_instances._fields.keys())
        for frame_index, frame in enumerate(frames):
            frame.requires_grad = False
            is_last = frame_index == len(frames) - 1
            if self.use_checkpoint and frame_index < len(frames) - 2:
                # Use checkpoint to reduce memory usage.
            else:
                frame = nested_tensor_from_tensor_list([frame])
                frame_res = self._forward_single_image(frame, track_instances)
            frame_res = self._post_process_single_image(frame_res, track_instances, is_last)

            track_instances = frame_res['track_instances']
            outputs['pred_logits'].append(frame_res['pred_logits'])
            outputs['pred_boxes'].append(frame_res['pred_boxes'])

        if not self.training:
            outputs['track_instances'] = track_instances
        else:
            outputs['losses_dict'] = self.criterion.losses_dict
        return outputs
```

As we can see, most of the work is done in the `_forward_single_image()` and `_post_process_single_image()` methods.

NOTE: if you want to know something about how the checkpoint works, you can check [Section 3.2. `CheckpointFunction`](#32-checkpointfunction).

### 2.1. `_forward_single_image()`

Based on the query position in the `track_instances`, the `MOTR` model predicts the logits of labels and boxes.

**Input:** `frame` and `track_instances`.

**Output:** Predicted logits of labels, predicted boxes, reference points, and hidden states.

```python
        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1], 'ref_pts': ref_pts_all[5]}
        if self.aux_loss:
            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)
        out['hs'] = hs[-1]
        return out
```

### 2.2 `_post_process_single_image()`

First, the satus of `track_instances` is updated.

```python
        track_instances.scores = track_scores
        track_instances.pred_logits = frame_res['pred_logits'][0]
        track_instances.pred_boxes = frame_res['pred_boxes'][0]
        track_instances.output_embedding = frame_res['hs'][0]
```

Then, the `RuntimeTrackerBase` is used to update the object id. Refer to [Section 3.2. `RuntimeTrackerBase`](#32-runtimetrackerbase) for more details.

```python
        # self.track_base is an instance of RuntimeTrackerBase.
        self.track_base.update(track_instances)
```

At last, the `track_instances` is passed to the query interaction module (QIM) to genenrate track queries for the next frame.

```python
        tmp = {}
        tmp['init_track_instances'] = self._generate_empty_tracks()
        tmp['track_instances'] = track_instances
        if not is_last:
            out_track_instances = self.track_embed(tmp)
            frame_res['track_instances'] = out_track_instances
        else:
            frame_res['track_instances'] = None
        return frame_res
```

Next section will go through the QIM in detail.

## 3. Query Interaction Module

![alt text](motr-qim.png)
_The Query Interaction Module in MOTR._

The QIM is used to generate track queries for the next frame based on the hidden states of the current frame. The implemenation of the QIM is in the `QueryInteractionModule` class.

### 3.1. `forward()`

```python
    def forward(self, data) -> Instances:
        active_track_instances = self._select_active_tracks(data)
        active_track_instances = self._update_track_embedding(active_track_instances)
        init_track_instances: Instances = data['init_track_instances']
        merged_track_instances = Instances.cat([init_track_instances, active_track_instances])
        return merged_track_instances
```

The `forward()` method first selects the active tracks from the `track_instances`. Then it updates the track embedding of the active tracks. Finally, it concatenates the active tracks and the new tracks to form the `merged_track_instances`.

### 3.2. `_select_active_tracks()`

```python
    def _select_active_tracks(self, data: dict) -> Instances:
        track_instances: Instances = data['track_instances']
        if self.training:
            active_idxes = (track_instances.obj_idxes >= 0) & (track_instances.iou > 0.5)
            active_track_instances = track_instances[active_idxes]
            # set -2 instead of -1 to ensure that these tracks will not be selected in matching.
            active_track_instances = self._random_drop_tracks(active_track_instances)
            if self.fp_ratio > 0:
                active_track_instances = self._add_fp_tracks(track_instances, active_track_instances)
        else:
            active_track_instances = track_instances[track_instances.obj_idxes >= 0]
```

`selec_active_tracks()` will select the active tracks based on the object id. If the model is in training mode, it will also randomly drop some tracks and add some false positive tracks.

### 3.3. `_update_track_embedding()`

To be understood...

Basic idea:
- Given the hidden states of the active tracks, update the track embedding and query position.
- `forward()` method return a merged `Instances` object that contains both the active tracks and the new tracks.

## 4. Data Structure and Functions

### 4.1. Track Instance

`Instances` class represents a list of instance in an image. It contains several fields, such as boxes, masks, labels, scores, object id etc. All values in the fields have the same length, which is the number of instances. 

```python
class Instances:
    def __init__(self, image_size: Tuple[int, int], **kwargs: Any):
        """
        Args:
            image_size (height, width): the spatial size of the image.
            kwargs: fields to add to this `Instances`.
        """
        self._image_size = image_size
        self._fields: Dict[str, Any] = {}
        for k, v in kwargs.items():
            self.set(k, v)
```

During the `forward()` method in `MOTR` class, the `Instances` are initialized by the `MOTR._generate_empty_tracks()` method. 

```python
def _generate_empty_tracks(self):
        track_instances = Instances((1, 1))
        num_queries, dim = self.query_embed.weight.shape  # (300, 512)
        device = self.query_embed.weight.device
        track_instances.ref_pts = self.transformer.reference_points(self.query_embed.weight[:, :dim // 2])
        track_instances.query_pos = self.query_embed.weight
        track_instances.output_embedding = torch.zeros((num_queries, dim >> 1), device=device)
        track_instances.obj_idxes = torch.full((len(track_instances),), -1, dtype=torch.long, device=device)
        track_instances.matched_gt_idxes = torch.full((len(track_instances),), -1, dtype=torch.long, device=device)
        track_instances.disappear_time = torch.zeros((len(track_instances), ), dtype=torch.long, device=device)
        track_instances.iou = torch.zeros((len(track_instances),), dtype=torch.float, device=device)
        track_instances.scores = torch.zeros((len(track_instances),), dtype=torch.float, device=device)
        track_instances.track_scores = torch.zeros((len(track_instances),), dtype=torch.float, device=device)
        track_instances.pred_boxes = torch.zeros((len(track_instances), 4), dtype=torch.float, device=device)
        track_instances.pred_logits = torch.zeros((len(track_instances), self.num_classes), dtype=torch.float, device=device)

        mem_bank_len = self.mem_bank_len
        track_instances.mem_bank = torch.zeros((len(track_instances), mem_bank_len, dim // 2), dtype=torch.float32, device=device)
        track_instances.mem_padding_mask = torch.ones((len(track_instances), mem_bank_len), dtype=torch.bool, device=device)
        track_instances.save_period = torch.zeros((len(track_instances), ), dtype=torch.float32, device=device)

        return track_instances.to(self.query_embed.weight.device)
```

Note that, `__setattr__` is defined in the `Instances` class to set the `_fields` attribute. So, `_generate_empty_tracks()` method sets lots of fields in the `Instances` object, such as `obj_idxes`, `matched_gt_idxes`, `disappear_time`, `iou`, `scores`, `track_scores`, `pred_boxes`, `pred_logits` etc. 

### 4.2. `RuntimeTrackerBase`

`RuntimeTrackerBase` is a class that is in charge of updating the object id during runtime.

#### 4.2.1. `__init__()`

`__init__()` method initializes the `RuntimeTrackerBase` object:

- `score_thresh`: the threshold of object entrance.
- `filter_score_thresh`: the threshold of object exit.
- `miss_tolerance`: if the object is not detected for `miss_tolerance` frames, it will be removed.
- `max_obj_id`: the maximum object id.

```python
    def __init__(self, score_thresh=0.7, filter_score_thresh=0.6, miss_tolerance=5):
        self.score_thresh = score_thresh
        self.filter_score_thresh = filter_score_thresh
        self.miss_tolerance = miss_tolerance
        self.max_obj_id = 0
```

#### 4.2.2. `update()`

```python
    def update(self, track_instances: Instances):
        # If the score of a track is larger than the score_thresh, refresh the disappear_time.
        track_instances.disappear_time[track_instances.scores >= self.score_thresh] = 0
        for i in range(len(track_instances)):
            # If the obj_id is -1 (which means it's a new object) and the score is larger than the score_thresh, assign a new obj_id.
            if track_instances.obj_idxes[i] == -1 and track_instances.scores[i] >= self.score_thresh:
                track_instances.obj_idxes[i] = self.max_obj_id
                self.max_obj_id += 1
            # If the obj_id is not -1 and the score is lower than the filter_score_thresh, increase the disappear_time.
            elif track_instances.obj_idxes[i] >= 0 and track_instances.scores[i] < self.filter_score_thresh:
                track_instances.disappear_time[i] += 1
                # If the disappear_time is larger than the miss_tolerance, remove the object.
                if track_instances.disappear_time[i] >= self.miss_tolerance:
                    track_instances.obj_idxes[i] = -1
```

### 4.3. `CheckpointFunction`

`CheckpointFunction` is a class that is used to reduce the memory usage of the model. In forward pass, it does not compute the gradients of the intermediate tensors. Instead, it recompute the gradients in the backward pass. 

#### 4.3.1. `torch.autograd.Function`

`torch.autograd.Function` is the base class for all autograd functions in PyTorch. You need to implement the `forward()` and `backward()` methods to define your autograd operation. Check the [official document](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) for more details.

`forward()` and `backward()` are both static methods. `ctx` is a context object that can be used to carry information between forward and backward calls. Check the [tutorial](https://brsoff.github.io/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html#pytorch-defining-new-autograd-functions) for more details. 

#### 4.3.2. `CheckpointFunction`

`CheckpointFunction` saves the run function and input tensors in the forward pass without computing the gradients. In the backward pass, it reruns the function with saved input tensors to compute the gradients.

Check [the repository](https://github.com/csrhddlam/pytorch-checkpoint) for more details.

```python
class CheckpointFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, run_function, length, *args):
        ctx.run_function = run_function
        ctx.input_tensors = list(args[:length])
        ctx.input_params = list(args[length:])
        with torch.no_grad():
            output_tensors = ctx.run_function(*ctx.input_tensors)
        return output_tensors

    def backward(ctx, *output_grads):
        # rerun the function with saved input tensors
```

### 4.4. Data Preparation

MOTR defines several interfaces to prepare the data for training and testing, eg. MOT17, DanceTrack, BDD100K etc. 

Here, we take the DanceTrack dataset as an example. Check [DanceTrack Dataset](https://github.com/DanceTrack/DanceTrack?tab=readme-ov-file) and [MOTR Dataset Preparation](https://github.com/megvii-research/MOTR?tab=readme-ov-file) for the format of the dataset. (Actually, I don't understand the format of MOT17 described in the MOTR readme file.)

The interface for the DanceTrack dataset is defined in the `class DetMOTDetection` in the `datasets/dance.py`. The code for the dataset appears to have been modified from MOT17, so there is some redundancy. The names of many variables are confusing. I'll try to explain the code in the following sections.

#### 4.4.1. `build()`

In the `main.py`, the `build()` method is used to build the dataset. 

```python
def build(image_set, args):
    root = Path(args.mot_path)
    assert root.exists(), f'provided MOT path {root} does not exist'
    dataset2transform = build_dataset2transform(args, image_set)
    if image_set == 'train':
        data_txt_path = args.data_txt_path_train
        dataset = DetMOTDetection(args, data_txt_path=data_txt_path, seqs_folder=root, dataset2transform=dataset2transform)
    if image_set == 'val':
        data_txt_path = args.data_txt_path_val
        dataset = DetMOTDetection(args, data_txt_path=data_txt_path, seqs_folder=root, dataset2transform=dataset2transform)
    return dataset
```

As you can see, `data_txt_path` is passed to the `DetMOTDetection` class. However, if you check the `./configs/r50_motr_train_dance.sh` file, you can figure out that `data_txt_path` is ignored in the `DetMOTDetection` class.

And for the `build_dataset2transform()` method, although it invokes the `make_transforms_for_mot17()` method, the transformation is still used for the DanceTrack dataset. Check `DetMOTDetection::__getitem__()` method for  details.

#### 4.4.2. `__init__()`

```python
    def __init__(self, args, data_txt_path: str, seqs_folder, dataset2transform):
        self.args = args
        self.dataset2transform = dataset2transform
        self.num_frames_per_batch = max(args.sampler_lengths)
        self.sample_mode = args.sample_mode
        self.sample_interval = args.sample_interval # 10
        self.video_dict = {}
        self.split_dir = os.path.join(args.mot_path, "DanceTrack", "train")
        ...
```

The path of the dataset is stored in the `split_dir` attribute. Since the `split_dir` is hard-coded, it is not necessary to pass the `data_txt_path` to the `DetMOTDetection` class. And the `DetMOTDetection` class is only used for the training set. 

Then `__init__()` method reads the ground truth files and stores the information in the `labels_full` attribute.

```python
        # __init__()
        ...

        self.labels_full = defaultdict(lambda : defaultdict(list))
        for vid in os.listdir(self.split_dir):
            # This if statement is used for MOT17 dataset.
            # Can be ignored for DanceTrack dataset.
            if 'DPM' in vid or 'FRCNN' in vid:
                print(f'filter {vid}')
                continue
            gt_path = os.path.join(self.split_dir, vid, 'gt', 'gt.txt')
            for l in open(gt_path):
                # t: frame index
                # i: object id
                # xywh: x, y, w, h
                # mark: 0 or 1, seem to be <conf>, why named `mark`?
                # label: seem to be world coordinates `x` in MOT gt format.
                #        why named `label`? 
                #        (whatever, it's not used in DanceTrack dataset)
                t, i, *xywh, mark, label = l.strip().split(',')[:8]
                t, i, mark, label = map(int, (t, i, mark, label))
                if mark == 0:   # Ignore
                    continue
                if label in [3, 4, 5, 6, 9, 10, 11]:  # Non-person
                    continue
                else:
                    crowd = False   # Not crowd, ignore for DanceTrack.
                x, y, w, h = map(float, (xywh))
                self.labels_full[vid][t].append([x, y, w, h, i, crowd])
        vid_files = list(self.labels_full.keys())  # list of video names
        ...
```

Then it prepares the indices of the video clips for the `__getitem__()` method.

```python
        # list of video clips, (video_name, start_frame).
        self.indices = []   # used in __getitem__()
        # dict of max frame index for each video.
        self.vid_tmax = {}
        for vid in vid_files:
            self.video_dict[vid] = len(self.video_dict)
            t_min = min(self.labels_full[vid].keys())
            t_max = max(self.labels_full[vid].keys()) + 1
            self.vid_tmax[vid] = t_max - 1
            # To ensure that the last frame can be used as the last frame of a batch.
            for t in range(t_min, t_max - self.num_frames_per_batch):
                self.indices.append((vid, t))

        # Init some attributes.
        self.sampler_steps: list = args.sampler_steps # [5, 9, 15]
        self.lengths: list = args.sampler_lengths # [2, 3, 4, 5]
        print("sampler_steps={} lenghts={}".format(self.sampler_steps, self.lengths))
        self.period_idx = 0
```

#### 4.4.3. `__getitem__()`

The `__getitem__()` method is used to get the data for training.

```python
    def __getitem__(self, idx):
        vid, f_index = self.indices[idx]
        # sample_indices() is used to sample the indices from start_frame with some interval.
        # incides: list of frame indices.
        indices = self.sample_indices(vid, f_index)
        # pre_continuous_frames() is used to prepare the images and targets for the indices.
        images, targets = self.pre_continuous_frames(vid, indices)
        dataset_name = targets[0]['dataset']
        transform = self.dataset2transform[dataset_name]
        if transform is not None:
            # Data augmentation.
            images, targets = transform(images, targets)
        gt_instances = []
        for img_i, targets_i in zip(images, targets):
            # Convert the targets to Instances object.
            gt_instances_i = self._targets_to_instances(targets_i, img_i.shape[1:3])
            gt_instances.append(gt_instances_i)
        return {
            'imgs': images,
            'gt_instances': gt_instances,
        }
```

`pre_continuouse_frames()` invokes `_pre_single_frame` to prepare the images and targets for the indices.

```python
    def _pre_single_frame(self, vid, idx: int):
        # Images should be stored in the 'img1' folder with the format like '00000001.jpg'.
        img_path = os.path.join(self.split_dir, vid, 'img1', f'{idx:08d}.jpg')
        img = Image.open(img_path)
        targets = {}
        w, h = img._size
        assert w > 0 and h > 0, "invalid image {} with shape {} {}".format(img_path, w, h)
        obj_idx_offset = self.video_dict[vid] * 100000  # 100000 unique ids is enough for a video.

        targets['dataset'] = 'MOT17'
        targets['boxes'] = []
        targets['area'] = []
        targets['iscrowd'] = []
        targets['labels'] = []
        targets['obj_ids'] = []
        targets['image_id'] = torch.as_tensor(idx)
        targets['size'] = torch.as_tensor([h, w])
        targets['orig_size'] = torch.as_tensor([h, w])
        for *xywh, id, crowd in self.labels_full[vid][idx]:
            targets['boxes'].append(xywh)
            targets['area'].append(xywh[2] * xywh[3])
            targets['iscrowd'].append(crowd)
            targets['labels'].append(0)
            targets['obj_ids'].append(id + obj_idx_offset)  # unique id for a video.

        targets['area'] = torch.as_tensor(targets['area'])
        targets['iscrowd'] = torch.as_tensor(targets['iscrowd'])
        targets['labels'] = torch.as_tensor(targets['labels'])
        targets['obj_ids'] = torch.as_tensor(targets['obj_ids'], dtype=torch.float64)
        targets['boxes'] = torch.as_tensor(targets['boxes'], dtype=torch.float32).reshape(-1, 4)
        targets['boxes'][:, 2:] += targets['boxes'][:, :2]
        return img, targets
```